from flask import Flask, render_template, request, redirect, url_for, flash, session, jsonify
from flask_sqlalchemy import SQLAlchemy
from werkzeug.security import generate_password_hash, check_password_hash
from datetime import datetime
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup  # dynamically update learning rate
import matplotlib.pyplot as plt
from googleapiclient.discovery import build
import random
from html import unescape

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your_secret_key'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.db'
db = SQLAlchemy(app)

#íšŒì› DB
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=True, nullable=False)
    password = db.Column(db.String(60), nullable=False)
    
#ì¼ê¸°ëª©ë¡ DB
class DiaryEntry(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    date = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)
    text = db.Column(db.Text, nullable=False)  # text ì»¬ëŸ¼ ì¶”ê°€
    analysis_result = db.Column(db.String(50), nullable=True)
    image_path = db.Column(db.String(100), nullable=True)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

# BERTSentenceTransform ìˆ˜ì •
class BERTSentenceTransform:
    r"""BERT style data transformation.

    Parameters
    ----------
    tokenizer : BERTTokenizer.
        Tokenizer for the sentences.
    max_seq_length : int.
        Maximum sequence length of the sentences.
    pad : bool, default True
        Whether to pad the sentences to maximum length.
    pair : bool, default True
        Whether to transform sentences or sentence pairs.
    """

    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):
        self._tokenizer = tokenizer
        self._max_seq_length = max_seq_length
        self._pad = pad
        self._pair = pair
        self._vocab = vocab

    def __call__(self, line):
        """Perform transformation for sequence pairs or single sequences.

        The transformation is processed in the following steps:
        - tokenize the input sequences
        - insert [CLS], [SEP] as necessary
        - generate type ids to indicate whether a token belongs to the first
        sequence or the second sequence.
        - generate valid length

        For sequence pairs, the input is a tuple of 2 strings:
        text_a, text_b.

        Inputs:
            text_a: 'is this jacksonville ?'
            text_b: 'no it is not'
        Tokenization:
            text_a: 'is this jack ##son ##ville ?'
            text_b: 'no it is not .'
        Processed:
            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'
            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
            valid_length: 14

        For single sequences, the input is a tuple of single string:
        text_a.

        Inputs:
            text_a: 'the dog is hairy .'
        Tokenization:
            text_a: 'the dog is hairy .'
        Processed:
            text_a: '[CLS] the dog is hairy . [SEP]'
            type_ids: 0     0   0   0  0     0 0
            valid_length: 7

        Parameters
        ----------
        line: tuple of str
            Input strings. For sequence pairs, the input is a tuple of 2 strings:
            (text_a, text_b). For single sequences, the input is a tuple of single
            string: (text_a,).

        Returns
        -------
        np.array: input token ids in 'int32', shape (batch_size, seq_length)
        np.array: valid length in 'int32', shape (batch_size,)
        np.array: input token type ids in 'int32', shape (batch_size, seq_length)

        """

        # convert to unicode
        text_a = line[0]
        if self._pair:
            assert len(line) == 2  # ë’¤ì˜ ì¡°ê±´ì´ Trueê°€ ì•„ë‹ˆë©´ AssertError ë°œìƒ
            text_b = line[1]

        tokens_a = self._tokenizer.tokenize(text_a)
        tokens_b = None

        if self._pair:
            tokens_b = self._tokenizer(text_b)

        if tokens_b:
            # Modifies `tokens_a` and `tokens_b` in place so that the total
            # length is less than the specified length.
            # Account for [CLS], [SEP], [SEP] with "- 3"
            self._truncate_seq_pair(tokens_a, tokens_b,
                                    self._max_seq_length - 3)
        else:
            # Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) > self._max_seq_length - 2:
                tokens_a = tokens_a[0:(self._max_seq_length - 2)]

        # The embedding vectors for `type=0` and `type=1` were learned during
        # pre-training and are added to the wordpiece embedding vector
        # (and position vector). This is not *strictly* necessary since
        # the [SEP] token unambiguously separates the sequences, but it makes
        # it easier for the model to learn the concept of sequences.

        # For classification tasks, the first vector (corresponding to [CLS]) is
        # used as as the "sentence vector". Note that this only makes sense because
        # the entire model is fine-tuned.
        #vocab = self._tokenizer.vocab
        vocab = self._vocab
        tokens = []
        tokens.append(vocab.cls_token)
        tokens.extend(tokens_a)
        tokens.append(vocab.sep_token)
        segment_ids = [0] * len(tokens)

        if tokens_b:
            tokens.extend(tokens_b)
            tokens.append(vocab.sep_token)
            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))

        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)

        # The valid length of sentences. Only real  tokens are attended to.
        valid_length = len(input_ids)

        if self._pad:
            # Zero-pad up to the sequence length.
            padding_length = self._max_seq_length - valid_length
            # use padding tokens for the rest
            input_ids.extend([vocab[vocab.padding_token]] * padding_length)
            segment_ids.extend([0] * padding_length)

        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\
            np.array(segment_ids, dtype='int32')

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
                 pad, pair):
        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)
        #transform = nlp.data.BERTSentenceTransform(
        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)
        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes = 5,   # ê°ì • í´ë˜ìŠ¤ ìˆ˜
                 dr_rate = None,
                 params = None):

        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p = dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
    
def get_kobert_model(model_path, vocab_file, ctx="cpu"):
                bertmodel = BertModel.from_pretrained(model_path)
                device = torch.device(ctx)
                bertmodel.to(device)
                bertmodel.eval()
                vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,
                                                         padding_token='[PAD]')
                return bertmodel, vocab_b_obj
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')

                # Torch GPU ì„¤ì •
device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
device = torch.device(device_type)

bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)
PATH = 'model_state_dict_test3.pt'  # state_dict ì €ì¥ ê²½ë¡œ
model = BERTClassifier(bertmodel,  dr_rate = 0.5)
model.load_state_dict(torch.load(PATH))
model.to(device)

def rader_chart(emotions,date_input, user):
    plt.switch_backend('agg')
    #emotions = [joy, confusion, anger, anxiety, sadness]
    emotion_names = ['ê¸°ì¨', 'ë‹¹í™©', 'ë¶„ë…¸', 'ë¶ˆì•ˆ', 'ìŠ¬í””']
    
    for i in range(len(emotions)):
        emotions[i] = int(emotions[i] * 100) + 10  # ì¸íŠ¸ë¡œ ë³€ê²½ í›„, ìµœì†Œê°’ìœ¼ë¡œ 5ë¥¼ ë¶€ì—¬
        emotions[i] = min(emotions[i], 100)  # ìµœëŒ€ê°’ì„ 100ìœ¼ë¡œ ì œí•œ

    today_emotion = {'Emotion':emotion_names,
                    'Value':emotions}
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜        
    df_emotion = pd.DataFrame(today_emotion)  
    plt.rc('font', family='Malgun Gothic')
    # ë ˆì´ë”ì°¨íŠ¸ ê·¸ë¦¬ê¸°
    label = df_emotion['Emotion']
    value = df_emotion['Value']
    
    num_vars = len(label)
    
    # ê° ë³€ìˆ˜ì˜ ê°ë„ ê³„ì‚°
    angles = [n / float(num_vars) * 2 * 3.14158 for n in range(num_vars)]
    angles += angles[:1]  # ì²˜ìŒ ì›ì†Œë¥¼ ë‹¤ì‹œ ì¶”ê°€í•˜ì—¬ ì™„ì „í•œ ì›ì„ ê·¸ë¦´ ìˆ˜ ìˆë„ë¡ í•¨
    
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    
    # ì°¨íŠ¸ ìƒì„±
    ax.plot(angles, value.tolist() + value.tolist()[:1], color='blue', alpha=0.25)  # value ë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì›ì†Œë¥¼ ì²˜ìŒì— ë‹¤ì‹œ ì¶”ê°€í•¨
    ax.fill(angles, value.tolist() + value.tolist()[:1], color='blue', alpha=0.25)  # ë‚´ë¶€ë¥¼ ì±„ì›€
    
    # ì™¸ê³½ ë‹¤ê°í˜• ì„¤ì •
    ax.set_theta_offset(3.14159 / 2)  # 90ë„ íšŒì „í•˜ì—¬ ë‹¤ê°í˜•ìœ¼ë¡œ ì„¤ì •
    ax.set_theta_direction(-1)  # ì‹œê³„ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‹¤ê°í˜• ì„¤ì •
    
    ax.set_yticklabels(['20%', '40%', '60%', '80%','100%'], fontsize=8)  # ë°˜ì§€ë¦„ ë¼ë²¨ ì œê±°
    plt.xticks(angles[:-1], label, color='black', size=10)  # ê°ë„ ìœ„ì¹˜ì— ê°ì • ë ˆì´ë¸” ì¶”ê°€

    # ê·¸ë˜í”„ í¬ê¸° 400 x 400 í”½ì…€ ì„¤ì •
    x = 400 / fig.dpi  # ê°€ë¡œ ê¸¸ì´ 
    y = 400 / fig.dpi  # ì„¸ë¡œ ê¸¸ì´ 
    fig.set_figwidth(x)
    fig.set_figheight(y)
    
    plt.rc('font', family='Malgun Gothic')
    plt.title('ê°ì • í™•ë¥  ì°¨íŠ¸',pad=20)
    image_path = "static/"+date_input+"radar_chart"+user+".png"
    plt.savefig(image_path, format='png')
    plt.close()
    image_data = date_input + 'radar_chart'+user+'.png'
    
    return image_data

# íšŒì›ê°€ì… ì¤‘ë³µì²´í¬ í•˜ëŠ” í•¨ìˆ˜
def is_username_taken(username):
    return User.query.filter_by(username=username).first() is not None

# ì‚¬ìš©ì ê°ì •ì— ë”°ë¥¸ ìŒì•…ì¶”ì²œ í•¨ìˆ˜
def music_recommend(emotion):
    # API í‚¤ ì„¤ì •
    api_key = 'AIzaSyCKW6KGpjqk_QExAIr781KeFxXUbjKog6A'

    # apië¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    youtube = build('youtube', 'v3', developerKey=api_key)

    # ê¸°ë¶„ì— ë”°ë¥¸ ê²€ìƒ‰ì–´ ëœë¤ ì¶”ì¶œ
    emotion_dict = {'ê¸°ì¨': 'ê¸°ì ', 'ë‹¹í™©': 'ë‹¹í™©í™œ', 'ë¶„ë…¸': 'í™”ê°€ ë‚ ', 'ìŠ¬í””': 'ìŠ¬í”Œ', 'ë¶ˆì•ˆ': 'ë¶ˆì•ˆí• ', 'ë‘ë ¤ì›€': 'ë‘ë ¤ìš¸'}
    today_emotion = emotion_dict[emotion]
    keyword = f"{today_emotion}ë•Œ ë“£ëŠ” ë…¸ë˜"
    search_keyword = keyword.replace(" ", "+")

    # youtubeì—ì„œ ê²€ìƒ‰
    search_response = youtube.search().list(
        q=search_keyword,
        part='id,snippet',
        maxResults=1
    ).execute()

    # ê²°ê³¼ ì¤‘ ì²«ë²ˆì§¸ ë™ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    first_video = search_response['items'][0]
    music_title =  unescape(first_video['snippet']['title'])
    music_img_url = first_video['snippet']['thumbnails']['default']['url']
    music_id = first_video['id']['videoId']
    music_url = f'https://www.youtube.com/watch?v={music_id}'

    return music_title, music_img_url, music_url

# ê°ì •ì— ë”°ë¼ ì˜ìƒ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜
def video_recommend(emotion):
    # API í‚¤ ì„¤ì •
    api_key = 'AIzaSyCKW6KGpjqk_QExAIr781KeFxXUbjKog6A'

    # apië¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    youtube = build('youtube', 'v3', developerKey=api_key)

    # ê¸°ë¶„ì— ë”°ë¥¸ ê²€ìƒ‰ì–´ ëœë¤ ì¶”ì¶œ
    emotion_dict = {'ê¸°ì¨': 'ê¸°ì ', 'ë‹¹í™©':'ë‹¹í™©í™œ', 'ë¶„ë…¸':'í™”ê°€ ë‚ ', 'ìŠ¬í””':'ìŠ¬í”Œ', 'ë¶ˆì•ˆ':'ë¶ˆì•ˆí• ', 'ë‘ë ¤ì›€':'ë‘ë ¤ìš¸'}
    today_emotion = emotion_dict[emotion]

    positive = ['ê¸°ì ']
    negative = ['ë‹¹í™©í• ', 'í™”ê°€ ë‚ ', 'ìŠ¬í”Œ', 'ë¶ˆì•ˆí• ', 'ë‘ë ¤ìš¸' ]

    if today_emotion in positive:
        keyword = "ì¦ê±°ìš´ ì˜ìƒ"
    else:
        keyword = f"{today_emotion}ë•Œ ë„ì›€ë˜ëŠ” ì˜ìƒ"

    search_keyword = keyword.replace(" ","+")

    # youtubeì—ì„œ ê²€ìƒ‰
    search_response = youtube.search().list(
        q=search_keyword,
        part='id,snippet',
        maxResults=1
    ).execute()

    # ê²°ê³¼ ì¤‘ ì²«ë²ˆì§¸ ë™ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    first_video = search_response['items'][0]
    video_title =  unescape(first_video['snippet']['title'])
    video_img_url = first_video['snippet']['thumbnails']['default']['url']
    video_id = first_video['id']['videoId']
    video_url = f'https://www.youtube.com/watch?v={video_id}'

    return video_title, video_img_url, video_url

# ê°ì •ì— ë”°ë¼ ì¡°ì–¸ì´ ë˜ëŠ” í•œë§ˆë””ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def get_one_word(emotion):
    joy = ['ë‹¹ì‹ ì´ ê¸°ë»í•˜ëŠ” ê±¸ ë³´ë‹ˆ ì €ë„ í–‰ë³µí•´ìš”!',
           'ë‹¹ì‹ ì˜ ê¸ì •ì  ì—ë„ˆì§€ê°€ ì£¼ë³€ì„ ë°ê²Œ ë§Œë“¤ì–´ìš”!', 
           'ë‹¹ì‹ ì´ ê¸°ë»í•˜ëŠ” ê²ƒì„ ë³´ëŠ”ê²ƒë§Œìœ¼ë¡œë„ ì €ë„ ê¸°ë¶„ì´ ì¢‹ì•„ìš”!']
    confusion = ['ë‹¹í™©í•˜ëŠ”ê±´ ë‹¹ì—°í•´ìš”. í•¨ê»˜ ì²œì²œíˆ ìˆ¨ì„ ê³ ë¥´ë©° í•´ê²°í•´ë´ìš”!',
                'ë‹¹í™©ìŠ¤ëŸ¬ìš´ ìˆœê°„ì—ë„ ìš°ë¦¬ëŠ” í•¨ê»˜ ìˆì–´ìš”. ì¡°ê¸ˆì”© ë¬¸ì œë¥¼ í•´ê²°í•´ ë‚˜ê°ˆ ê±°ì˜ˆìš”!',
                'ì–´ìƒ‰í•œ ìƒí™©ì´ë¼ ë‹¹í™©í•  ìˆ˜ ìˆì–´ìš”. ì¡°ê¸ˆì”© í¸ì•ˆí•´ì§€ë„ë¡ ë…¸ë ¥í•´ë³´ì•„ìš”!']
    anger = ['ì´ ìˆœê°„ì— ëŒ€í•œ ë¶„ë…¸ëŠ” ì ì‹œ ì¼ì‹œì ì¸ ê°ì •ì¼ ìˆ˜ ìˆì–´ìš”. ì¡°ê¸ˆ ì‹œê°„ì„ ê°€ì ¸ë„ ì¢‹ì•„ìš”!',
            'ë§ˆìŒì„ ê°€ë¼ì•‰íˆê³  ëª…ë£Œí•œ ìƒí™©íŒë‹¨ì„ ìœ„í•´ ê¹Šê²Œ ìˆ¨ì„ ë“¤ì´ë§ˆì…”ë³´ì„¸ìš”!',
            'ë‹¹ì‹ ì˜ ê°ì •ì€ ë‹¹ì‹ ì˜ ì„±ì¥ê³¼ ì´í•´ë¥¼ ìœ„í•œ ì¼ë¶€ì¼ ë¿ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìì‹ ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆì„ê±°ì˜ˆìš”']
    sadness = ['ìì‹ ì— ëŒ€í•œ ìë¹„ë¡œìš´ íƒœë„ë¥¼ ì·¨í•˜ì„¸ìš”. ìŠ¬í””ì€ ìì‹ ì„ ì´í•´í•˜ê³  ì¹˜ìœ í•  ê¸°íšŒë¥¼ ì£¼ëŠ” ê²ƒ ì…ë‹ˆë‹¤.!',
              'í˜ë“¤ ë•ŒëŠ” ì£¼ìœ„ì˜ ëˆ„êµ°ê°€ì™€ í•¨ê»˜í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”!',
              'ìŠ¬í””ì€ ì¼ì‹œì ì¸ ê°ì •ì…ë‹ˆë‹¤. ê·¸ê²ƒì€ ë‹¹ì‹ ì´ ì§€ë‚˜ì¹  ìˆ˜ ìˆëŠ” ì‹œê°„ì´ë¼ëŠ” ê²ƒì„ ìƒê¸°í•˜ì„¸ìš”!']
    anxiety = ['ì§€ê¸ˆì€ ì–´ë ¤ìš´ ìƒí™©ì¼ ìˆ˜ ìˆì§€ë§Œ, ìš°ë¦¬ê°€ í•¨ê»˜ í•´ê²°í•  ìˆ˜ ìˆì„ê±°ì˜ˆìš”!',
              'ë‹¹ì‹ ì€ ì¶©ë¶„íˆ ê°•í•´ìš”! ì´ ì–´ë ¤ì›€ì„ ê·¹ë³µí•  ìˆ˜ ìˆì„ê±°ì˜ˆìš”!',
              'ê±±ì •ê±°ë¦¬ë¥¼ í•˜ë‚˜ì”© ì°¨ë¶„í•˜ê²Œ ë‹¤ë£¨ì–´ê°€ë©´, ì˜ í•´ê²°í•  ìˆ˜ ìˆì„ê±°ì˜ˆìš”!']
    fear = ['ë‘ë ¤ì›€ì€ ìƒˆë¡œìš´ ì‹œì‘ì„ ê°€ë¡œë§‰ì§€ë§Œ, ê·¸ê²ƒì„ ì´ê²¨ë‚´ë©´ ìƒˆë¡œìš´ ì„±ì¥ê³¼ ê¸°íšŒê°€ ì˜¬ê±°ì˜ˆìš”!',
            'ìì‹ ì˜ ë§ˆìŒì„ ëŒ€í•  ìš©ê¸°ë¥¼ ê°€ì ¸ì£¼ì„¸ìš”. ì‰½ê²Œ ì§€ë‚˜ì¹˜ëŠ” ì¼ì´ ì•„ë‹ ìˆ˜ ìˆì§€ë§Œ, ê·¸ê²ƒì´ ìƒˆë¡œìš´ ê¸¸ì„ ì—´ì–´ì¤„ê±°ì˜ˆìš”!',
            'ê°•í•¨ì€ ì‰½ê²Œ ë³¼ ìˆ˜ ì—†ëŠ” ìš©ê¸°ì™€ ê²°ì‹¬ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ë¯¸ ê°•í•˜ê³  ìš©ê°í•œ ì‚¬ëŒì…ë‹ˆë‹¤!']

    emo_kr_eng = {'ê¸°ì¨': joy, 'ë‹¹í™©': confusion, 'ë¶„ë…¸': anger, 'ìŠ¬í””': sadness, 'ë¶ˆì•ˆ': anxiety, 'ë‘ë ¤ì›€': fear}
    return f"{random.choice(emo_kr_eng[emotion])}"


@app.route('/')  # ë©”ì¸í™”ë©´
def index():
    with app.app_context():
        if 'user_id' in session:
            user = User.query.get(session['user_id'])
            return render_template('index.html', user=user)
        return redirect(url_for('login'))

@app.route('/register', methods=['GET', 'POST'])  # íšŒì›ê°€ì…
def register():
    with app.app_context():
        if request.method == 'POST':
            username = request.form['username']
            password = request.form['password']

            if is_username_taken(username):
                flash('ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‚¬ìš©ìëª…ì…ë‹ˆë‹¤.', 'danger')
                return redirect(url_for('register'))

            hashed_password = generate_password_hash(password, method='pbkdf2:sha256')
            new_user = User(username=username, password=hashed_password)
            db.session.add(new_user)
            db.session.commit()
            flash('ê³„ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!', 'success')
            return redirect(url_for('login'))
        return render_template('register.html')

@app.route('/login', methods=['GET', 'POST'])  # ë¡œê·¸ì¸í™”ë©´
def login():
    with app.app_context():
        if request.method == 'POST':
            username = request.form['username']
            password = request.form['password']
            user = User.query.filter_by(username=username).first()
            if user and check_password_hash(user.password, password):
                session['user_id'] = user.id
                flash('ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤!', 'success')
                return redirect(url_for('diary_list'))
            else:
                flash('ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'danger')
        return render_template('login.html')

@app.route('/diary_list')
def diary_list():
    with app.app_context():
        if 'user_id' in session:
            
            user = User.query.get(session['user_id'])

            entries = DiaryEntry.query.filter_by(user_id=user.id).order_by(DiaryEntry.date.desc()).all()

            return render_template('diary_list.html', entries=entries)
        else:
            return redirect(url_for('login'))
        
@app.route('/delete_entry/<int:entry_id>', methods=['POST'])
def delete_entry(entry_id):
    with app.app_context():
        if 'user_id' in session:
            user = User.query.get(session['user_id'])
            entry = DiaryEntry.query.get(entry_id)

            # í•´ë‹¹ ì¼ê¸°ë¥¼ ì‘ì„±í•œ ì‚¬ìš©ìì¸ì§€ í™•ì¸
            if entry and entry.user_id == user.id:
                db.session.delete(entry)
                db.session.commit()
                return jsonify(success=True)
            else:
                return jsonify(success=False, error="Permission denied")
        else:
            return jsonify(success=False, error="User not logged in")

@app.route('/logout',methods=['POST'])  # 
def logout():
    with app.app_context():
        session.pop('user_id', None)
        return redirect(url_for('login'))

@app.route('/predict', methods=['POST'])
def predict():
    with app.app_context():
        if 'user_id' in session:
            user = User.query.get(session['user_id'])
            try:
                entry_text = request.json['text']
                entry_date = datetime.strptime(request.json['date'], '%Y-%m-%d')
                # ì…ë ¥ ë¬¸ì¥ ì „ì²˜ë¦¬ ë° ê°ì • ë¶„ë¥˜
                def predict(predict_sentence):

                    data = [predict_sentence, '0']
                    dataset_another = [data]

                    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, 64, True, False)
                    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=64, num_workers=0)

                    model.eval()

                    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
                        token_ids = token_ids.long().to(device)
                        segment_ids = segment_ids.long().to(device)
                        valid_length= valid_length

                        out = model(token_ids, valid_length, segment_ids)


                        test_eval=[]
                        for i in out:
                            logits=i
                            logits = logits.detach().cpu().numpy()
                            probabilities = F.softmax(torch.from_numpy(logits), dim=0)

                            if np.argmax(logits) == 0:
                                test_eval.append('"ê¸°ì¨"ì´')
                                sentence = "ì˜¤ëŠ˜ì€ğŸ˜Š" + test_eval[0] + " ëŠê»´ì§€ë„¤ìš”"
                            elif np.argmax(logits) == 1:
                                test_eval.append('"ë‹¹í™©"ì´')
                                sentence = "ì˜¤ëŠ˜ì€ğŸ˜³" + test_eval[0] + " ëŠê»´ì§€ë„¤ìš”"
                            elif np.argmax(logits) == 2:
                                test_eval.append('"ë¶„ë…¸"ê°€')
                                sentence = "ì˜¤ëŠ˜ì€ğŸ˜¡" + test_eval[0] + " ëŠê»´ì§€ë„¤ìš”"
                            elif np.argmax(logits) == 3:
                                test_eval.append('"ë¶ˆì•ˆ"ì´')
                                sentence = "ì˜¤ëŠ˜ì€ğŸ˜¨" + test_eval[0] + " ëŠê»´ì§€ë„¤ìš”"
                            elif np.argmax(logits) == 4:
                                test_eval.append('"ìŠ¬í””"ì´')
                                sentence = "ì˜¤ëŠ˜ì€ğŸ˜­" + test_eval[0] + " ëŠê»´ì§€ë„¤ìš”"
                        #í™•ë¥  logits 
                        #sentence = "ì˜¤ëŠ˜ì€ "  + test_eval[0]  + " ëŠê»´ì§€ë„¤ìš”."
                        return sentence, probabilities
                global prediction
                global image_path
                prediction, probability = predict(entry_text)
                prob = probability.tolist()
                image_path = rader_chart(prob,entry_date.strftime('%Y-%m-%d'), str(user.id))
                
                new_entry = DiaryEntry(date=entry_date, text=entry_text, user_id=user.id, analysis_result=prediction, image_path=image_path)
                db.session.add(new_entry)
                db.session.commit()
                print(probability)

                return jsonify(success=True)
            except Exception as e:
                print("Error predicting entry:", str(e))
                return jsonify(success=False, error=str(e))
        else:
            return jsonify(success=False, error="User not logged in")

@app.route('/result')
def result():
    
    return render_template('result.html',prediction = prediction, image_path = image_path)

@app.route('/chat')
def chat():
    return render_template('chat.html')

@app.route('/write',methods=['POST'])
def write():
    return render_template('index.html')

@app.route('/recommend_music')
def recommend_music():
    # ê°ì •ì„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ìì˜ ê°ì • ì •ë³´ë¥¼ ë°›ì•„ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
    emotion = prediction[5:7]

    # ìŒì•… ì¶”ì²œ í•¨ìˆ˜ í˜¸ì¶œ
    music_title, music_img_url, music_url = music_recommend(emotion)

    # ì¶”ì²œëœ ìŒì•… ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì „ë‹¬
    return jsonify(emotion=emotion, music_title= music_title, music_img_url=music_img_url, music_url=music_url)

@app.route('/recommend_video')
def recommend_video():
    # ê°ì •ì„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ìì˜ ê°ì • ì •ë³´ë¥¼ ë°›ì•„ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
    emotion = prediction[5:7]

    # ì˜ìƒ ì¶”ì²œ í•¨ìˆ˜ í˜¸ì¶œ
    video_title, video_img_url, video_url = video_recommend(emotion)

    # ì¶”ì²œëœ ì˜ìƒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì „ë‹¬
    return jsonify(emotion=emotion, video_title=video_title, video_img_url=video_img_url, video_url=video_url)

@app.route('/get_one_word')
def get_one_word_route():
    emotion = prediction[5:7]
    one_word = get_one_word(emotion)
    return jsonify(one_word=one_word)

if __name__ == '__main__':
    with app.app_context():
        
        db.create_all()
        app.run(debug=True)